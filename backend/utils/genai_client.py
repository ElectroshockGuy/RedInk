"""Google GenAI å®¢æˆ·ç«¯å°è£…"""
import time
import random
import logging
from functools import wraps
from google import genai
from google.genai import types

# å¯¼å…¥ç»Ÿä¸€çš„é”™è¯¯è§£æå‡½æ•°
from ..generators.google_genai import parse_genai_error

logger = logging.getLogger(__name__)


def retry_on_429(max_retries=3, base_delay=2):
    """429 é”™è¯¯è‡ªåŠ¨é‡è¯•è£…é¥°å™¨ï¼ˆå¸¦æ™ºèƒ½é”™è¯¯è§£æï¼‰"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_error = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_error = e
                    error_str = str(e).lower()

                    # ä¸å¯é‡è¯•çš„é”™è¯¯ç±»å‹
                    non_retryable = [
                        "401", "unauthenticated",  # è®¤è¯é”™è¯¯
                        "403", "permission_denied", "forbidden",  # æƒé™é”™è¯¯
                        "404", "not_found",  # èµ„æºä¸å­˜åœ¨
                        "invalid_argument",  # å‚æ•°é”™è¯¯
                        "safety", "blocked", "filter",  # å®‰å…¨è¿‡æ»¤
                    ]

                    should_retry = True
                    for keyword in non_retryable:
                        if keyword in error_str:
                            should_retry = False
                            break

                    if not should_retry:
                        # ç›´æ¥æŠ›å‡ºï¼Œä¸é‡è¯•
                        raise Exception(parse_genai_error(e))

                    # å¯é‡è¯•çš„é”™è¯¯
                    if attempt < max_retries - 1:
                        if "429" in error_str or "resource_exhausted" in error_str:
                            wait_time = (base_delay ** attempt) + random.uniform(0, 1)
                            print(f"[é‡è¯•] é‡åˆ°èµ„æºé™åˆ¶ï¼Œ{wait_time:.1f}ç§’åé‡è¯• (å°è¯• {attempt + 2}/{max_retries})")
                        else:
                            wait_time = min(2 ** attempt, 10) + random.uniform(0, 1)
                            print(f"[é‡è¯•] è¯·æ±‚å¤±è´¥ï¼Œ{wait_time:.1f}ç§’åé‡è¯• (å°è¯• {attempt + 2}/{max_retries})")
                        time.sleep(wait_time)
                        continue

                    # é‡è¯•æ¬¡æ•°è€—å°½
                    raise Exception(parse_genai_error(e))

            # ç†è®ºä¸Šä¸ä¼šåˆ°è¿™é‡Œï¼Œä½†ä¿é™©èµ·è§
            raise Exception(parse_genai_error(last_error))
        return wrapper
    return decorator


class GenAIClient:
    """GenAI å®¢æˆ·ç«¯å°è£…ç±»ï¼ˆå·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨ GoogleGenAIGeneratorï¼‰"""

    def __init__(self, api_key: str = None, base_url: str = None):
        self.api_key = api_key
        if not self.api_key:
            raise ValueError(
                "Google Cloud API Key æœªé…ç½®ã€‚\n"
                "è§£å†³æ–¹æ¡ˆï¼šåœ¨ç³»ç»Ÿè®¾ç½®é¡µé¢ç¼–è¾‘è¯¥æœåŠ¡å•†ï¼Œå¡«å†™ API Key"
            )

        # æ„å»ºå®¢æˆ·ç«¯å‚æ•°
        client_kwargs = {"api_key": self.api_key}

        # å¦‚æœæœ‰ base_urlï¼Œä½¿ç”¨ http_options
        if base_url:
            client_kwargs["http_options"] = {
                "base_url": base_url,
                "api_version": "v1beta"
            }

        # é»˜è®¤ä½¿ç”¨ Gemini API (vertexai=False)ï¼Œå› ä¸ºå¤§å¤šæ•°ç”¨æˆ·ä½¿ç”¨ Google AI Studio çš„ API Key
        # Vertex AI éœ€è¦ OAuth2 è®¤è¯ï¼Œä¸æ”¯æŒ API Key
        client_kwargs["vertexai"] = False

        self.client = genai.Client(**client_kwargs)

        # é»˜è®¤å®‰å…¨è®¾ç½®ï¼šå…¨éƒ¨å…³é—­
        self.default_safety_settings = [
            types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="OFF"),
            types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="OFF"),
            types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="OFF"),
            types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="OFF"),
        ]

    def generate_text_stream(
        self,
        prompt: str,
        model: str = "gemini-3-pro-preview",
        temperature: float = 1.0,
        max_output_tokens: int = 8000,
        use_search: bool = False,
        use_thinking: bool = False,
        images: list = None,
        system_prompt: str = None,
        **kwargs
    ):
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬ï¼ˆç”Ÿæˆå™¨å‡½æ•°ï¼‰

        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦
            max_output_tokens: æœ€å¤§è¾“å‡º token
            use_search: æ˜¯å¦ä½¿ç”¨æœç´¢
            use_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            images: å›¾ç‰‡åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯

        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        logger.debug(f"ğŸ”„ GenAI æµå¼ç”Ÿæˆå¼€å§‹: model={model}")
        parts = [types.Part(text=prompt)]

        if images:
            for img_data in images:
                if isinstance(img_data, bytes):
                    parts.append(types.Part(
                        inline_data=types.Blob(
                            mime_type="image/png",
                            data=img_data
                        )
                    ))

        contents = [
            types.Content(
                role="user",
                parts=parts
            )
        ]

        config_kwargs = {
            "temperature": temperature,
            "top_p": 0.95,
            "max_output_tokens": max_output_tokens,
            "safety_settings": self.default_safety_settings,
        }

        # æ·»åŠ æœç´¢å·¥å…·
        if use_search:
            config_kwargs["tools"] = [types.Tool(google_search=types.GoogleSearch())]

        # æ·»åŠ æ€è€ƒé…ç½®
        if use_thinking:
            config_kwargs["thinking_config"] = types.ThinkingConfig(thinking_level="HIGH")

        generate_content_config = types.GenerateContentConfig(**config_kwargs)

        chunk_count = 0
        for chunk in self.client.models.generate_content_stream(
            model=model,
            contents=contents,
            config=generate_content_config,
        ):
            if not chunk.candidates or not chunk.candidates[0].content or not chunk.candidates[0].content.parts:
                continue
            if chunk.text:
                chunk_count += 1
                logger.debug(f"ğŸ“¥ GenAI chunk #{chunk_count}: {len(chunk.text)} å­—ç¬¦")
                yield chunk.text

        logger.debug(f"âœ… GenAI æµå¼ç”Ÿæˆå®Œæˆï¼Œå…± {chunk_count} ä¸ª chunk")

    @retry_on_429(max_retries=3, base_delay=2)
    def generate_text(
        self,
        prompt: str,
        model: str = "gemini-3-pro-preview",
        temperature: float = 1.0,
        max_output_tokens: int = 8000,
        use_search: bool = False,
        use_thinking: bool = False,
        images: list = None,
        system_prompt: str = None,
        **kwargs
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬

        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦
            max_output_tokens: æœ€å¤§è¾“å‡º token
            use_search: æ˜¯å¦ä½¿ç”¨æœç´¢
            use_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            images: å›¾ç‰‡åˆ—è¡¨ï¼ˆæš‚ä¸æ”¯æŒï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆæš‚ä¸æ”¯æŒï¼‰

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        parts = [types.Part(text=prompt)]

        if images:
            for img_data in images:
                if isinstance(img_data, bytes):
                    parts.append(types.Part(
                        inline_data=types.Blob(
                            mime_type="image/png",
                            data=img_data
                        )
                    ))

        contents = [
            types.Content(
                role="user",
                parts=parts
            )
        ]

        config_kwargs = {
            "temperature": temperature,
            "top_p": 0.95,
            "max_output_tokens": max_output_tokens,
            "safety_settings": self.default_safety_settings,
        }

        # æ·»åŠ æœç´¢å·¥å…·
        if use_search:
            config_kwargs["tools"] = [types.Tool(google_search=types.GoogleSearch())]

        # æ·»åŠ æ€è€ƒé…ç½®
        if use_thinking:
            config_kwargs["thinking_config"] = types.ThinkingConfig(thinking_level="HIGH")

        generate_content_config = types.GenerateContentConfig(**config_kwargs)

        result = ""
        for chunk in self.client.models.generate_content_stream(
            model=model,
            contents=contents,
            config=generate_content_config,
        ):
            if not chunk.candidates or not chunk.candidates[0].content or not chunk.candidates[0].content.parts:
                continue
            result += chunk.text

        return result

    @retry_on_429(max_retries=5, base_delay=3)  # å›¾ç‰‡ç”Ÿæˆé‡è¯•æ›´å¤šæ¬¡
    def generate_image(
        self,
        prompt: str,
        model: str = "gemini-3-pro-image-preview",
        aspect_ratio: str = "3:4",
        temperature: float = 1.0,
    ) -> bytes:
        """
        ç”Ÿæˆå›¾ç‰‡

        Args:
            prompt: æç¤ºè¯
            model: æ¨¡å‹åç§°
            aspect_ratio: å®½é«˜æ¯”
            temperature: æ¸©åº¦

        Returns:
            å›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®
        """
        contents = [
            types.Content(
                role="user",
                parts=[types.Part(text=prompt)]
            )
        ]

        generate_content_config = types.GenerateContentConfig(
            temperature=temperature,
            top_p=0.95,
            max_output_tokens=32768,
            response_modalities=["TEXT", "IMAGE"],
            safety_settings=self.default_safety_settings,
            image_config=types.ImageConfig(
                aspect_ratio=aspect_ratio,
                output_mime_type="image/png",
            ),
        )

        image_data = None
        for chunk in self.client.models.generate_content_stream(
            model=model,
            contents=contents,
            config=generate_content_config,
        ):
            if chunk.candidates and chunk.candidates[0].content and chunk.candidates[0].content.parts:
                for part in chunk.candidates[0].content.parts:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡æ•°æ®
                    if hasattr(part, 'inline_data') and part.inline_data:
                        image_data = part.inline_data.data
                        break

        if not image_data:
            raise ValueError(
                "âŒ å›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼šAPI è¿”å›ä¸ºç©º\n\n"
                "ã€å¯èƒ½åŸå› ã€‘\n"
                "1. æç¤ºè¯è§¦å‘äº†å®‰å…¨è¿‡æ»¤ï¼ˆæœ€å¸¸è§ï¼‰\n"
                "2. æ¨¡å‹ä¸æ”¯æŒå½“å‰çš„å›¾ç‰‡ç”Ÿæˆè¯·æ±‚\n"
                "3. ç½‘ç»œä¼ è¾“è¿‡ç¨‹ä¸­æ•°æ®ä¸¢å¤±\n\n"
                "ã€è§£å†³æ–¹æ¡ˆã€‘\n"
                "1. ä¿®æ”¹æç¤ºè¯ï¼Œé¿å…æ•æ„Ÿå†…å®¹ï¼š\n"
                "   - é¿å…æ¶‰åŠæš´åŠ›ã€è¡€è…¥ã€è‰²æƒ…ç­‰å†…å®¹\n"
                "   - é¿å…æ¶‰åŠçœŸå®äººç‰©ï¼ˆæ˜æ˜Ÿã€æ”¿æ²»äººç‰©ç­‰ï¼‰\n"
                "   - ä½¿ç”¨æ›´ä¸­æ€§ã€ç§¯æçš„æè¿°\n"
                "2. å°è¯•ç®€åŒ–æç¤ºè¯\n"
                "3. æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•"
            )

        return image_data


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_client_instance = None

def get_genai_client() -> GenAIClient:
    """è·å–å…¨å±€ GenAI å®¢æˆ·ç«¯å®ä¾‹"""
    global _client_instance
    if _client_instance is None:
        _client_instance = GenAIClient()
    return _client_instance
